1. In this assignment, we didn't ask you to support phrasal
queries, which is a feature that is typically supported in web
search engines. Describe how you would support phrasal search in
conjunction with the VSM model. A sketch of the algorithm is
sufficient. (For those of you who like a challenge, please go
ahead and implement this feature in your submission but clearly
demarcate it in your code and allow this feature to be turned on
or off using the command line switch "-x" (where "-x" means to
turn on the extended processing of phrasal queries). We will
give a small bonus to submissions that achieve this
functionality correctly).

One possible way of implementing phrasal search would be the
following:

	1. Treat the phrasal part of the query as another term
	in the query.

	2. Evaluate the number of files that have the phrase.
	   This will be the df_t

	3. After OR-ing the different terms, perform the same kind of
	evaluation as before, still treating the phrasal query as a
	single term.

	4. Since we have the tf_t, and the df_t, it is possible to
	compute the cosine similarity of the query and the document, and
	so we can again compute the score for each document

This method, however, does not allow for any variation in the
phrases. For example, a document with "Singapore's National
University", would not be considered with a query of "National
University of Singapore". One possible way to rectify this, is
to retrieve the document as we normally would, but then give a
penalty when we see missing words or permutations of the
original phrase. One possibility would be to check the squared
error of the term positions, based on the first term in the
phrase.

Query :		"National University of Singapore"
Document:	"Singapore's National University"

after preprocessing

Query :		"national university singapore"
Document:	"singapore national university"

first word is national, so for a national at position i,
university should be at i+1, and document should be at i + 2.
In this case, "singapore" is at i-1, so cost incurred:

	error = ((i+2) - (i-1))^2 = 9

This should then be subject to some type of tunable coefficient
to get better ranking results based on user feedback.

	score = cosine_sim - coeff*Error


2. Describe how your search engine reacts to long documents and
long queries as compared to short documents and queries. Is the
normalization you use sufficient to address the problems (see
Section 6.4.4 for a hint)? In your judgement, is the lnc.ltc
scheme (n.b., not the ranking scheme you were asked to
implement) sufficient for retrieving documents from the
Reuters-21578 collection?

Longer documents and long queries tend to be ranked higher than
short queries and short documents. This is due to the fact that
the longer documents tend to have a higher number of occurences
of query terms. Shorter terms with shorter queries mean that
each term in the query has a higher weightage as well. The
normalisation used is insufficient, as it has a bias towards
longer documents.

For the Reuters corpus, many of the articles are of a single
topic, and the terms tend to be homogeneous throughout the
document. This tends to mean that the number of unique terms per
document are fairly standard, and since lnc.ltc normalises the
word occurences based on the vector length, this has already
been accounted for. As such, I believe that the lnc.ltc scheme
is sufficient.


3. Do you think zone or field parametric indices would be useful
for practical search in the Reuters collection? Note: the
Reuters collection does have metadata for each article but the
quality of the metadata is not uniform, nor are the metadata
classifications uniformly applied (some documents have it, some
don't).

If we make the assumption that a search through the Reuters
corpora is to find _every_ occurence of a certain search term,
then using parametric indeces on a corpus like this with
incomplete metadata is potentially dangerous, as important
information may not be retrieved.

However, if the search need not be complete, and the corpus has
a relatively high percentage of documents with the metadata in
question, and document retrieval is not expected to be a 100%,
then indexing the corpus may prove to be useful in narrowing down
search queries.


